## business logic
- handle spreadsheets and pdfs, compare different pipeline processing methods

## data
- can start from dirty spreadsheet data from https://www.kaggle.com/datasets/shivavashishtha/dirty-excel-data

## data-pipeline (假想)
- two stage, first find boundaries and blocks, then normalize data

## pattern
- create a pdf module and a spreadsheet module, each containing multiple parsing methods
- may use factor design or just simple direct constructor call (factory corresponds to the data pipelines better, so factory should be optimal)
- can start with the following structure

rag_app/
│── parsers/
│   ├── __init__.py
│   ├── base_parser.py       # abstract base class (interface)
│   ├── spreadsheet_parser.py
│   ├── pdf_parser.py
│   └── parser_factory.py    # factory implementation
│
│── pipelines/
│   ├── __init__.py
│   ├── spreadsheet_pipeline.py   # optional: pipeline logic unique to spreadsheets
│   ├── pdf_pipeline.py           # optional: pipeline logic unique to PDFs
│
│── main.py                  # entrypoint for your RAG app



## test parsing

- goal is to get blocks of continuous content as accurately as possible
- realising that in between each processed block NaN values represent spacing
- so we can construct an algorithm to find boundaries and then extract them
- **assumption**: there are subtables with and without header

## main flow
- python3 quick-poc.py

## visualize parsing results
- streamlit run simple-rag/parsing_sample_code/streamlit_app.py
